{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "647a0fd1",
   "metadata": {},
   "source": [
    "Chapter-4 focuses on RNN, Gated Recurrent Units (GRU), Long Short Term Memory(LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e7088",
   "metadata": {},
   "source": [
    "### RNN\n",
    "\n",
    "RNN is used in 2 senses:\n",
    "\n",
    "    . General - Any NN built using a Recurrent Computational Graph\n",
    "    . Special - Concrete architectute of NN\n",
    "    \n",
    "2 major disadvantages of fully connected NN:\n",
    "\n",
    "    . Fixed input - can only accept an input sequence of fixed length\n",
    "    . Does not perceive the vector as an ordered sequence\n",
    "    \n",
    "RNN has concept of hidden state. Hidden state can be treated as internal memory. The hidden state does not try to remember all past values of the seuqnce but only thier effect. Because of the internal memory RNNs can rember important things about thier input, which allow them to be very accurate in predicting future values.\n",
    "\n",
    "RNN takes the internal memory(hidden state), combines it with an event, and returns the new updated internal memory(hidden state).So when it makes a decision, it considers the current input and also what is has learned from the inputs it received previously.\n",
    "\n",
    "    ht = tanh(WitXt + bih + Whhh(t-1) + bhh)\n",
    "    \n",
    "RNN cell combines information about the current value of the sequence Xi an the previously hidden state hi-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71932937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self\n",
    "                , hidden_sie\n",
    "                , in_size = 1\n",
    "                , out_size = 1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size = in_size\n",
    "                         , hidden_size = hidden_size\n",
    "                         , batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, out_size)\n",
    "    def forward(self, x, h = None):\n",
    "        out, _ = self.rnn(x, h)\n",
    "        last_hidden_state = out[:, -1]\n",
    "        out = self.fc(last_hidden_state)\n",
    "        return out, last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "## Parameters\n",
    "\n",
    "# length of sliding window\n",
    "features = 240\n",
    "# length of test dataset\n",
    "test_ts_len = 300\n",
    "# size of RNN hidden state\n",
    "rnn_hidden_size = 24\n",
    "# optimizer learning rate\n",
    "learning_rate = 0.02\n",
    "training_epochs = 500\n",
    "\n",
    "# Prepare datasets for training\n",
    "\n",
    "ts = get_aep_timeseries()\n",
    "scaler = MinMaxScaler()\n",
    "scaled_ts = scaler.fit_transform(ts)\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = get_training_datasets(scaled_ts\n",
    "                                                                       , features\n",
    "                                                                       , test_ts_len\n",
    "                                                                      )\n",
    "\n",
    "\n",
    "# Initializing the model\n",
    "model = RNN(hidden_size = rnn_hidden_size)\n",
    "model.train()\n",
    "\n",
    "#Training\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "best_model = None\n",
    "min_val_loss = sys.maxsize\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "for t in range(training_epochs):\n",
    "    prediction, _ = model(x_train)\n",
    "    loss = mse_loss(prediction, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    val_prediction, _ = model(x_val)\n",
    "    val_loss = mes_loss(val_predicton, y_val)\n",
    "    training_loss.append(loss.item())\n",
    "    validation_loss.append(val_loss.item())\n",
    "    if val_loss.item() < min_val_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        min_val_loss = val_loss.item()\n",
    "    if t%50 == 0:\n",
    "        print(f'epoch {t}: train - {round(loss.item(),4)},' f'val: - {round(val_loss.item(), 4)}')\n",
    "        \n",
    "# evaluation\n",
    "best_model.eval()\n",
    "_, h_list = best_model(x_val)\n",
    "# warm hidden state\n",
    "h = (h_list[-1, :].unsqueeze(-2))\n",
    "predicted = []\n",
    "for test_seq in x_test.tolist():\n",
    "    x = torch.Tensor(data=[test_seq])\n",
    "    #passing hidden state through each iteration\n",
    "    y, h = best_model(x, h.unsqueeze(-2))\n",
    "    unscaled = scaler.inverse_transform(np.array(y.item()).reshape(-1,1))[0][0]\n",
    "    predict.append(unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5e1a66",
   "metadata": {},
   "source": [
    "RNN significant disadvantage:\n",
    "\n",
    "    . Suffers from vanishing gradient problems due to computational complexity\n",
    "    . Training too slow\n",
    "    . Difficult to store long-term information in RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bf391",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit\n",
    "\n",
    "    . Store long term information\n",
    "    . Way to store long-term information in a hidden state is to restrict hidden state updates on each iteration.\n",
    "     \n",
    "$r_t$ is an output of a simple NN on two input vectors: $h_(t-1)$ and $x_t$. The variable $r_t$ is responsible for fogetting unimportant parts in the hidden state of GRU.\n",
    "\n",
    "    0.1 - Value is close to 0, and highly likely corresponding hidden state value will be forgotten\n",
    "    0 - Means that the corresponding hidden state value will be forgotten\n",
    "    0.8 - Highly likely corresponding hidden state value will be passed to a further hidden state\n",
    "    1 - The corresponding hidden state value will be passed to a futher hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f61d77b",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "    . RNN has a hidden state that can be treated as an internal memory of the input sequence.\n",
    "    . RNN recalculates the hidden state after processing each new input values recurrently.\n",
    "    . RNN suffers from vanishing gradient problem\n",
    "    . RNN updates a hidden state on each iteration. This it has no long-term memory.\n",
    "    . GRU implements the reset gate, which declines some updates in a hidden memory.\n",
    "    . LSTM passes 2 vectors through each iteration: hidden state and cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec62a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c486ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c999310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758ddef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82c99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
